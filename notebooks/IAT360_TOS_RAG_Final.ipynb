{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQwsblbEEZIX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f72546f3ba08480b92ea3de3ebb7c8b6",
      "2725d163573647ea85fadb67b7aeebde",
      "cc2adf03fa1349db900f399ec1ac9dda",
      "d533c8a383cf4a5d82a812d866b1c241",
      "026c9539222f4926a8ab053e31526b03",
      "ae425a188c6f4f779996fdba9190c4b5",
      "783915a1ecb14ec1af25b18ea7d43a87",
      "2f22883c6c0d4c38a08c03a35154a218",
      "9799635fe8074d59b32df098d9bb9a7f",
      "58493eb8c1394a79840a5ce88b6682c3",
      "608a669f6f8d4e61aff423cbb8386ae6",
      "bd7d7684100f4161b42d52166226165a",
      "1623bc2a37c64d0a89625f6adb23cbac",
      "af07909651244815be4663ee1876a6ba",
      "67ba18dbee6c4d80b4a4895dfc889720",
      "3c3bc9b07a694c5f863efdfb0258724f",
      "0e461e5063314a9b83d51a90246b61d4",
      "d96536e13c884bdfb9fdeeeb7e6ab103",
      "54eec318f6544c89b528a1f0906f1a2b",
      "9c7ebfa1697046bfab52c1b6f97d599d",
      "8966e324bbc84c22904c019be9ac7e39",
      "e3e479566afa4924979a22972658256c",
      "e0fc257850304550b2e94417f0be435f",
      "b1b45e95f66946e1a571366b70dfaf91",
      "8cc5351e90414792bb5c7474e194331b",
      "595abd79d11741c0a6a61b971f06dbba",
      "0b38386252404406a4f0d720a057fecd",
      "c3fa08b79b784cfeaf1954f49c080258",
      "339d1cd071114dedb0e96fe034c33a90",
      "92d82b9aa4964539bd56e7a2bb9f74a5",
      "03e2746df90b4cec99f3e99d7437b89a",
      "89e2354bc07d4ac08748d7014ce83ef2",
      "f11d8273db2440a29a61d0c411398de7",
      "b18dadf326b944188811dafb89bf9a86",
      "904af9e190424ae28e11620112f9462e",
      "55ced61b90dc4a7cbbfe570fe3501da3",
      "bba9472764984bc78dd31d1a46982da9",
      "94f25deee7d44986aed0cb59b1ab6c77",
      "aa93f5aa48674364ab1d66a3dc813d5d",
      "58b7e279c1ee4cd3bc9586f967a207ea",
      "7c2bfe5a890440bd867e4b00eedd9860",
      "036dbb2fd9e04113aa10b74c5228aca2",
      "cfd8618bb9f44c6696b830186b9367b9",
      "af43570995a348c79cc01469c5ab95d2",
      "c3103c005ea342cd9bd3f8e1eaa9bb9b",
      "426a77917d1e41608760c340f75839eb",
      "2295b7ca182c48d3ac4aaca4d3a58d7c",
      "9924c5dcdb5f48b0a7a449dc3876b11b",
      "c7680ae1f077446db73b83969021275a",
      "fde9bc699a63492a8eb554fa3be54f08",
      "b7028470a17442e19a8be6d1467f772a",
      "95f02bd7d82d47d99811c8712591477f",
      "d36e2f07b4af493dac953134281c274d",
      "864d2411fb29497585b0234b9c4f915d",
      "713d848178ef4087b42ab58bf1a5ac85",
      "4aebc37ccc5a4304977c679438836d04",
      "012e647d05ea4e1c81bbd14bcfdaee97",
      "ba3fec8f93904e15bccbf06a0d9c2783",
      "9cb712a6d8564bcd9d313c34e0880560",
      "8f3c5a2f532a4a3fb501d33d2a31ccce",
      "4ce733df13394716a4c0f81b7d9404b1",
      "c070848ff4f843848ce8478fba10d772",
      "954d6ba6441347769be500f912baccdf",
      "6d7a729c48b04593a16d1e5d2fae49e9",
      "7be3cede9437439abe411f2ef5e09b22",
      "855f96f1f74449deb7eba4c8808fc314",
      "500e68c432f84a5bb7ebcf38f2a28dde",
      "c7a383b6c4114223bc2292d1558a6eaf",
      "987510f5e4704a4d8358e5975a7042aa",
      "7c1d422739d749ff8b2d1990181c43d8",
      "61eeff231d5b4e54ad04f836c5791657",
      "3614d5223134469e8a1cd04de6d746a1",
      "769fa8bfd6714b6685b5f430b75e39cb",
      "82f8880f0bc24133b4cdc9c9ef3ade0a",
      "acf919afa9b5405eb0de291c68e3541d",
      "0abcd142a1e642f3bfb9d74d250c5b2f",
      "09ddc30676a74b04b179a7a56ec6b7da",
      "9a2af222541440b28c5ac17ca4bcef69",
      "7017f7c093e04d518561e6be0416b733",
      "110877a656744df7b434f635a23690e3",
      "e8eb9e1ebb194d51a1a7ee03502ab18a",
      "8d2417369b9d4d77bb14e95d209dba6c",
      "ae7450d2349f4ce691fc9b2868509fe2",
      "b7723b1bf8694b97976760d318ac33e1",
      "4525990591a546b99d574c232298e495",
      "d9476a0fa7814ee293d729047a8d0fb6",
      "515a4612d2a6483bade737c21cdca547",
      "7f32d034a63f48248159e31462a49f81",
      "ff70a6373259487c94c9233814a09eb1",
      "b99efc9a143d48609b5cd7d4b9be5ec9",
      "533e4d28482d4aa1b817ebf33ac156af",
      "7599eb2f86bd4ae79b818c9d70e41575",
      "e2606e2a7a524c4bb5e3d4bf822a1c74",
      "dddb90c5b65e40c89b2c4baeb788e552",
      "6c5ccae2e8604b94b298349123d9e3ac",
      "bb5b0a14656f44378abb02f8719e8613",
      "0fb788ad95b64ba388e798a07f48a52f",
      "9658b5fe01ce45dcb961857e90cf5172",
      "c08960ae676b4078a58d99cf20a27a19",
      "902067952f634b1faa60e1bff939c542",
      "602f6f0937f4488aacf47a50e6ea274a",
      "19f9279463104140a65a57d7277ddb29",
      "5bed82815b7644d19b58874a719a3e10",
      "74bb7869d9814bc08e8483731cdfd0ae",
      "2caf075460af4996af5d62495a1c51b5",
      "566e2043689a4404b36dcce47c089a01",
      "4d6467bcd4354bcda5bae554f48f2a3e",
      "00a8265fd84c41eb91c996e9c8a70f48",
      "79bc8267f5dd49beb59598a90602854e",
      "278afc36ff0048be90e78499c484fb74",
      "7c9022fce03a4721a1acbbbc3abf43f6",
      "b1b5235ff03b47f28499dfb7fad65569",
      "bd711e9e215541b09341d8097e0fdfbd",
      "5ae182a8d9c64ccfa354409b3659dd84",
      "9f1fe57e823144e0bad78209cadd1f07",
      "8b6c767996b247c3a5665e2d9e0bc569",
      "a1e05c03ea724ec7bc5aaccd82d2ab10",
      "8a26b658494b4af9a0abb6aad506492b",
      "7cb4f0baa1724e6e846f278b19e2444d",
      "b597ffa203f64342a97494342aa3cdc5",
      "1ecaf6e8e42a4fceae42a4d132688b6d"
     ]
    },
    "id": "sQwsblbEEZIX",
    "outputId": "0a8c274a-6149-44a6-d3af-3f20b80ba73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udce6 Installing packages...\n",
      "\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Cannot install langchain-community==0.0.13, langchain-text-splitters==0.2.0, langchain-text-splitters==0.2.1, langchain-text-splitters==0.2.2, langchain-text-splitters==0.2.4, langchain-text-splitters==0.3.0, langchain-text-splitters==0.3.1, langchain-text-splitters==0.3.10, langchain-text-splitters==0.3.11, langchain-text-splitters==0.3.2, langchain-text-splitters==0.3.3, langchain-text-splitters==0.3.4, langchain-text-splitters==0.3.5, langchain-text-splitters==0.3.6, langchain-text-splitters==0.3.7, langchain-text-splitters==0.3.8, langchain-text-splitters==0.3.9, langchain-text-splitters==1.0.0 and langchain==0.1.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\u2705 Installation complete!\n",
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATASET FROM HUGGING FACE\n",
      "============================================================\n",
      "\n",
      "\ud83d\udce5 Downloading OPP-115 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Dataset loaded successfully!\n",
      "   - Train set: 2185 examples\n",
      "   - Validation set: 550 examples\n",
      "   - Test set: 697 examples\n",
      "\n",
      "\ud83d\udccb Sample document:\n",
      "------------------------------------------------------------\n",
      "Text preview:  \"\"Contact Us\"\" Link If you contact us through the \"\"Contact Us\"\" link on this site, we ask you for information such as your first name, e-mail address, and other information, so we can respond to your questions and comments. You may choose to provide additional information as well. ...\n",
      "\n",
      "Available fields: ['text', 'label']\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "STEP 2: PREPARING DOCUMENTS\n",
      "============================================================\n",
      "\n",
      "Processing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2185/2185 [00:00<00:00, 34835.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 550/550 [00:00<00:00, 21670.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 697/697 [00:00<00:00, 27230.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Total documents prepared: 3432\n",
      "\ud83d\udcbe Saved sample_documents.json for inspection\n",
      "\n",
      "============================================================\n",
      "STEP 3: CHUNKING DOCUMENTS FOR RETRIEVAL\n",
      "============================================================\n",
      "\n",
      "\u2702\ufe0f Splitting text into searchable chunks...\n",
      "\n",
      "\u2705 Created 3929 searchable chunks\n",
      "   Average chunk size: ~401 characters\n",
      "\n",
      "============================================================\n",
      "STEP 4: LOADING EMBEDDING MODEL FROM HUGGING FACE\n",
      "============================================================\n",
      "\n",
      "\ud83e\udd17 Loading: sentence-transformers/all-MiniLM-L6-v2\n",
      "   - Model size: 80MB\n",
      "   - Embedding dimensions: 384\n",
      "   - License: Apache 2.0 (free for all uses)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipython-input-3403892186.py:112: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72546f3ba08480b92ea3de3ebb7c8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7d7684100f4161b42d52166226165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fc257850304550b2e94417f0be435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18dadf326b944188811dafb89bf9a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3103c005ea342cd9bd3f8e1eaa9bb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aebc37ccc5a4304977c679438836d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500e68c432f84a5bb7ebcf38f2a28dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2af222541440b28c5ac17ca4bcef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff70a6373259487c94c9233814a09eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902067952f634b1faa60e1bff939c542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9022fce03a4721a1acbbbc3abf43f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Embedding model loaded successfully!\n",
      "\n",
      "============================================================\n",
      "STEP 5: BUILDING FAISS VECTOR DATABASE\n",
      "============================================================\n",
      "\n",
      "\ud83d\udd28 Creating embeddings for all chunks...\n",
      "   (This may take 2-3 minutes for 3,000+ chunks)\n",
      "\n",
      "\u2705 Vector database created successfully!\n",
      "   Total vectors: 3929\n",
      "\n",
      "============================================================\n",
      "STEP 6: TESTING RETRIEVAL\n",
      "============================================================\n",
      "\n",
      "\ud83e\uddea Running test queries...\n",
      "\n",
      "Query: 'Can the company sell my personal data to third parties?'\n",
      "\u2705 Found 2 relevant chunks\n",
      "   Top result preview: Note, that we will not share your Personally Identifiable Information with third parties for their marketing purposes without obtaining your prior con...\n",
      "\n",
      "Query: 'Do they use my content to train AI models?'\n",
      "\u2705 Found 2 relevant chunks\n",
      "   Top result preview: Our automated systems analyze your content (including emails) to provide you personally relevant product features, such as customized search results, ...\n",
      "\n",
      "Query: 'Can I delete all my data from their servers?'\n",
      "\u2705 Found 2 relevant chunks\n",
      "   Top result preview: Remember that even after you cancel your account, copies of some information from your account may remain viewable in some circumstances where, for ex...\n",
      "\n",
      "Query: 'Am I forced into arbitration instead of court?'\n",
      "\u2705 Found 2 relevant chunks\n",
      "   Top result preview: . Should you file any arbitration claims, or any administrative or legal actions without first having attempted to resolve the matter by mediation, th...\n",
      "\n",
      "============================================================\n",
      "STEP 7: SAVING DATABASE\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcbe Saving FAISS database to disk...\n",
      "\u2705 Database saved to: faiss_index_tos_hf/\n",
      "\u2705 Metadata saved\n",
      "\n",
      "\ud83d\udce6 Creating zip file for download...\n",
      "\n",
      "============================================================\n",
      "\u2705 SUCCESS! DATABASE READY FOR DEPLOYMENT\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcc1 FILES CREATED:\n",
      "   1. faiss_index_tos_hf/          (the vector database folder)\n",
      "   2. faiss_index_tos_hf.zip       (download this for your Streamlit app)\n",
      "   3. sample_documents.json        (for inspection)\n",
      "\n",
      "\ud83c\udfaf NEXT STEPS:\n",
      "   1. Download 'faiss_index_tos_hf.zip' from the Files panel (left sidebar)\n",
      "   2. Unzip it on your computer\n",
      "   3. Upload the 'faiss_index_tos_hf' folder to your GitHub repo\n",
      "   4. Deploy your Streamlit app!\n",
      "\n",
      "\ud83d\udca1 TIP: The vector database is now ready to use with either:\n",
      "   - Option A: 100% Free (Hugging Face models only)\n",
      "   - Option B: Hybrid (HF embeddings + OpenAI GPT-3.5)\n",
      "\n",
      "\n",
      "\ud83d\udcca File sizes:\n",
      "7.8M\tfaiss_index_tos_hf\n",
      "6.0M\tfaiss_index_tos_hf.zip\n"
     ]
    }
   ],
   "source": [
    "# 1. INSTALL DEPENDENCIES\n",
    "!pip install -q sentence-transformers faiss-cpu datasets langchain==0.1.0 langchain-community==0.0.13 langchain-text-splitters\n",
    "\n",
    "print(\"Installation complete!\\n\")\n",
    "\n",
    "# 2. IMPORTS\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 3. LOAD DATA FROM HUGGING FACE\n",
    "print(\"=\" * 30)\n",
    "print(\"LOADING DATASETS FROM HUGGING FACE\")\n",
    "\n",
    "# Dataset 1: OPP-115\n",
    "print(\"\\nDownloading Dataset 1: OPP-115 (alzoubi36/opp_115)...\")\n",
    "dataset_1 = load_dataset(\"alzoubi36/opp_115\")\n",
    "\n",
    "# Dataset 2: PrivacyPolicy\n",
    "print(\"\\nDownloading Dataset 2: PrivacyPolicy (sjsq/PrivacyPolicy)...\")\n",
    "dataset_2 = load_dataset(\"sjsq/PrivacyPolicy\")\n",
    "\n",
    "\n",
    "# Dataset 3: Kaggle ToSDR\n",
    "print(\"\\nLoading Dataset 3: Kaggle ToSDR (sonu1607/tosdr-terms-of-service-corpus)...\")\n",
    "# Since we don't have kaggle CLI, we check if file exists locally\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "kaggle_files = [f for f in os.listdir('.') if f.endswith('.csv') or f.endswith('.json')]\n",
    "dataset_3 = None\n",
    "\n",
    "# Try to find the file\n",
    "target_file = None\n",
    "for f in kaggle_files:\n",
    "    if 'tosdr' in f.lower() or 'corpus' in f.lower():\n",
    "        target_file = f\n",
    "        break\n",
    "\n",
    "if target_file:\n",
    "    print(f\"   Found local file: {target_file}\")\n",
    "    try:\n",
    "        if target_file.endswith('.csv'):\n",
    "            dataset_3 = pd.read_csv(target_file)\n",
    "        else:\n",
    "            dataset_3 = pd.read_json(target_file)\n",
    "        print(f\"   - Kaggle ToSDR: {len(dataset_3)} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error loading file: {e}\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f  Kaggle dataset file not found locally.\")\n",
    "    print(\"   Please download the dataset from https://www.kaggle.com/datasets/sonu1607/tosdr-terms-of-service-corpus\")\n",
    "    print(\"   and place it in the same directory as this notebook (e.g., 'tosdr_corpus.csv').\")\n",
    "print(f\"\\nDatasets loaded successfully!\")\n",
    "print(f\"   - OPP-115 Train: {len(dataset_1['train'])} examples\")\n",
    "print(f\"   - PrivacyPolicy Train: {len(dataset_2['train'])} examples\")\n",
    "\n",
    "# 4. CONVERT TO LANGCHAIN DOCUMENTS \n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"PREPARING DOCUMENTS\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Process Dataset 1 (OPP-115)\n",
    "print(\"\\nProcessing OPP-115 dataset...\")\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    if split in dataset_1:\n",
    "        print(f\"  - Processing {split} split...\")\n",
    "        for i, item in enumerate(tqdm(dataset_1[split])):\n",
    "            doc = Document(\n",
    "                page_content=item['text'],\n",
    "                metadata={\n",
    "                    'source': 'opp_115',\n",
    "                    'split': split,\n",
    "                    'index': i,\n",
    "                    'original_id': f\"opp_{split}_{i}\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "# Process Dataset 2 (sjsq/PrivacyPolicy)\n",
    "print(\"\\nProcessing sjsq/PrivacyPolicy dataset...\")\n",
    "for split in dataset_2.keys():\n",
    "    print(f\"  - Processing {split} split...\")\n",
    "    for i, item in enumerate(tqdm(dataset_2[split])):\n",
    "        # Normalize column names (this dataset uses 'Text')\n",
    "        text_content = item.get('Text', item.get('text', ''))\n",
    "        if text_content and text_content != 'Null':\n",
    "             doc = Document(\n",
    "                page_content=text_content,\n",
    "                metadata={\n",
    "                    'source': 'sjsq_privacy_policy',\n",
    "                    'split': split,\n",
    "                    'index': i,\n",
    "                    'filename': item.get('filename', 'unknown')\n",
    "                }\n",
    "            )\n",
    "             documents.append(doc)\n",
    "\n",
    "\n",
    "# Process Dataset 3 (Kaggle ToSDR)\n",
    "if dataset_3 is not None:\n",
    "    print(\"\\nProcessing Kaggle ToSDR dataset...\")\n",
    "    # Inspect columns to find text column\n",
    "    text_col = None\n",
    "    for col in dataset_3.columns:\n",
    "        if 'text' in col.lower() or 'content' in col.lower() or 'terms' in col.lower() or 'service' in col.lower():\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col:\n",
    "        print(f\"   Using column '{text_col}' for text content\")\n",
    "        for i, row in tqdm(dataset_3.iterrows(), total=len(dataset_3)):\n",
    "            text_content = row[text_col]\n",
    "            if isinstance(text_content, str) and len(text_content) > 100:\n",
    "                doc = Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata={\n",
    "                        'source': 'kaggle_tosdr',\n",
    "                        'index': i,\n",
    "                        'filename': target_file\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    else:\n",
    "        print(f\"   \u26a0\ufe0f  Could not find a suitable text column in {dataset_3.columns}\")\n",
    "print(f\"\\nTotal documents prepared: {len(documents)}\")\n",
    "# 5. CHUNK THE DOCUMENTS\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"CHUNKING DOCUMENTS FOR RETRIEVAL\")\n",
    "\n",
    "print(\"\\nSplitting text into searchable chunks...\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\n\u2705 Created {len(chunks)} searchable chunks\")\n",
    "print(f\"   Average chunk size: ~{sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "\n",
    "# 6. CREATE EMBEDDINGS MODEL\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"LOADING EMBEDDING MODEL FROM HUGGING FACE\")\n",
    "\n",
    "print(\"\\nLoading: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"   - Model size: 80MB\")\n",
    "print(\"   - Embedding dimensions: 384\")\n",
    "print(\"   - License: Apache 2.0 (free for all uses)\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"\\nEmbedding model loaded successfully!\")\n",
    "\n",
    "# 7. BUILD VECTOR DATABASE\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"BUILDING FAISS VECTOR DATABASE\")\n",
    "\n",
    "print(\"\\n\ud83d\udd28 Creating embeddings for all chunks...\")\n",
    "print(\"   (This may take 2-3 minutes for 3,000+ chunks)\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"\\nVector database created successfully!\")\n",
    "print(f\"   Total vectors: {vectorstore.index.ntotal}\")\n",
    "\n",
    "# 8. TEST THE DATABASE\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"TESTING RETRIEVAL\")\n",
    "\n",
    "test_queries = [\n",
    "    \"Can the company sell my personal data to third parties?\",\n",
    "    \"Do they use my content to train AI models?\",\n",
    "    \"Can I delete all my data from their servers?\",\n",
    "    \"Am I forced into arbitration instead of court?\"\n",
    "]\n",
    "\n",
    "print(\"\\nRunning test queries...\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    print(f\"Found {len(results)} relevant chunks\")\n",
    "    print(f\"   Top result preview: {results[0].page_content[:150]}...\")\n",
    "    print()\n",
    "\n",
    "# 9. SAVE THE DATABASE\n",
    "print(\"=\" * 30)\n",
    "print(\"SAVING DATABASE\")\n",
    "\n",
    "print(\"\\n\ud83d\udcbe Saving FAISS database to disk...\")\n",
    "vectorstore.save_local(\"faiss_index_tos_hf\")\n",
    "\n",
    "print(\"Database saved to: faiss_index_tos_hf/\")\n",
    "\n",
    "# 10. CREATE METADATA FILE\n",
    "metadata = {\n",
    "    \"dataset\": \"alzoubi36/opp_115\",\n",
    "    \"total_documents\": len(documents),\n",
    "    \"total_chunks\": len(chunks),\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"embedding_dimension\": 384,\n",
    "    \"chunk_size\": 800,\n",
    "    \"chunk_overlap\": 100,\n",
    "    \"vector_database\": \"FAISS\",\n",
    "    \"date_created\": \"2024-12\",\n",
    "    \"license\": \"Apache 2.0\",\n",
    "    \"use_case\": \"Terms of Service Analysis - IAT 360\"\n",
    "}\n",
    "\n",
    "with open('faiss_index_tos_hf/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved\")\n",
    "\n",
    "# 11. ZIP FOR DOWNLOAD\n",
    "print(\"\\nCreating zip file for download...\")\n",
    "!zip -r -q faiss_index_tos_hf.zip faiss_index_tos_hf\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"SUCCESS! DATABASE READY FOR DEPLOYMENT\")\n",
    "\n",
    "# Display file sizes\n",
    "print(\"\\n\ud83d\udcca File sizes:\")\n",
    "!du -sh faiss_index_tos_hf\n",
    "!du -sh faiss_index_tos_hf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rwtakRLIFDZS",
   "metadata": {
    "id": "rwtakRLIFDZS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}