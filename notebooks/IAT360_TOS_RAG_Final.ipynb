{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "pRnheGRdX9g9",
      "metadata": {
        "id": "pRnheGRdX9g9"
      },
      "outputs": [],
      "source": [
        "# 1. INSTALL DEPENDENCIES\n",
        "!pip install -q sentence-transformers faiss-cpu datasets langchain>=0.1.0 langchain-community>=0.0.34 langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sQwsblbEEZIX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQwsblbEEZIX",
        "outputId": "ea0becff-4b58-4bd2-ad92-a42e4e3c11dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installation complete!\n",
            "\n",
            "==============================\n",
            "LOADING DATASETS FROM HUGGING FACE\n",
            "\n",
            "Downloading Dataset 1: OPP-115 (alzoubi36/opp_115)...\n",
            "\n",
            "Downloading Dataset 2: PrivacyPolicy (sjsq/PrivacyPolicy)...\n",
            "\n",
            "Loading Dataset 3: Kaggle ToSDR (sonu1607/tosdr-terms-of-service-corpus)...\n",
            "   Found zip file: tos.zip\n",
            "   Unzipping...\n",
            "   Unzip complete.\n",
            "   Found local directory: text\n",
            "   Found 9491 text files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading Kaggle files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9491/9491 [00:00<00:00, 16790.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - Kaggle ToSDR: 9488 documents loaded\n",
            "\n",
            "Datasets loaded successfully!\n",
            "   - OPP-115 Train: 2185 examples\n",
            "   - PrivacyPolicy Train: 1789 examples\n",
            "\n",
            "==============================\n",
            "PREPARING DOCUMENTS\n",
            "\n",
            "Processing OPP-115 dataset...\n",
            "  - Processing train split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2185/2185 [00:00<00:00, 24022.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Processing validation split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 18084.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Processing test split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 697/697 [00:00<00:00, 23275.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing sjsq/PrivacyPolicy dataset...\n",
            "  - Processing train split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1789/1789 [00:00<00:00, 8864.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total documents prepared: 14709\n",
            "\n",
            "==============================\n",
            "CHUNKING DOCUMENTS FOR RETRIEVAL\n",
            "\n",
            "Splitting text into searchable chunks...\n",
            "\n",
            "âœ… Created 343120 searchable chunks\n",
            "   Average chunk size: ~579 characters\n",
            "\n",
            "==============================\n",
            "LOADING EMBEDDING MODEL FROM HUGGING FACE\n",
            "\n",
            "Loading: sentence-transformers/all-MiniLM-L6-v2\n",
            "   - Using Device: CUDA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3330005816.py:167: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embedding model loaded successfully!\n",
            "\n",
            "==============================\n",
            "BUILDING FAISS VECTOR DATABASE\n",
            "\n",
            "ðŸ”¨ Creating embeddings for all chunks...\n",
            "   (Processing...)\n",
            "\n",
            "Vector database created successfully!\n",
            "   Total vectors: 343120\n",
            "\n",
            "==============================\n",
            "TESTING RETRIEVAL\n",
            "\n",
            "Running test queries...\n",
            "\n",
            "Query: 'Can the company sell my personal data to third parties?'\n",
            "Found 2 relevant chunks\n",
            "   Top result preview: YOUR PERSONAL DATA WILL NOT BE SOLD. Nor will Your Personal Data be licensed or disclosed to unaffiliated third-parties, except in connection with the...\n",
            "\n",
            "Query: 'Do they use my content to train AI models?'\n",
            "Found 2 relevant chunks\n",
            "   Top result preview: 50.2. You and your End Users are responsible for all decisions made, advice given, actions taken, and failures to take action based on your use of AI ...\n",
            "\n",
            "Query: 'Can I delete all my data from their servers?'\n",
            "Found 2 relevant chunks\n",
            "   Top result preview: Delete your data: You also hold the right to have your personal data deleted.\n",
            "This is sometimes known as the 'right to be forgotten'.\n",
            "To request that ...\n",
            "\n",
            "Query: 'Am I forced into arbitration instead of court?'\n",
            "Found 2 relevant chunks\n",
            "   Top result preview: Mandatory Arbitration...\n",
            "\n",
            "==============================\n",
            "SAVING DATABASE\n",
            "\n",
            "ðŸ’¾ Saving FAISS database to disk...\n",
            "Database saved to: faiss_index_tos_hf/\n",
            "Metadata saved\n",
            "\n",
            "Creating zip file for download...\n",
            "\n",
            "==============================\n",
            "SUCCESS! DATABASE READY FOR DEPLOYMENT\n",
            "\n",
            "ðŸ“Š File sizes:\n",
            "736M\tfaiss_index_tos_hf\n",
            "538M\tfaiss_index_tos_hf.zip\n"
          ]
        }
      ],
      "source": [
        "# 2. IMPORTS\n",
        "import os \n",
        "import json\n",
        "import zipfile \n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 3. LOAD DATA FROM HUGGING FACE\n",
        "print(\"=\" * 30)\n",
        "print(\"LOADING DATASETS FROM HUGGING FACE\")\n",
        "\n",
        "# Dataset 1: OPP-115\n",
        "print(\"\\nDownloading Dataset 1: OPP-115 (alzoubi36/opp_115)...\")\n",
        "dataset_1 = load_dataset(\"alzoubi36/opp_115\")\n",
        "\n",
        "# Dataset 2: PrivacyPolicy\n",
        "print(\"\\nDownloading Dataset 2: PrivacyPolicy (sjsq/PrivacyPolicy)...\")\n",
        "dataset_2 = load_dataset(\"sjsq/PrivacyPolicy\")\n",
        "\n",
        "# Dataset 3: Kaggle ToSDR\n",
        "print(\"\\nLoading Dataset 3: Kaggle ToSDR (sonu1607/tosdr-terms-of-service-corpus)...\")\n",
        "\n",
        "# Check for zip file and unzip if needed\n",
        "zip_files = [f for f in os.listdir('.') if f.endswith('.zip') and ('tos' in f.lower() or 'corpus' in f.lower())]\n",
        "\n",
        "if zip_files:\n",
        "    print(f\"   Found zip file: {zip_files[0]}\")\n",
        "    print(\"   Unzipping...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_files[0], 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(\"   Unzip complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error unzipping: {e}\")\n",
        "\n",
        "# Check for directory containing text files\n",
        "kaggle_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and ('text' in d.lower() or 'tosdr' in d.lower())]\n",
        "target_dir = None\n",
        "\n",
        "# Try to find the directory\n",
        "for d in kaggle_dirs:\n",
        "    # Check if it contains txt files\n",
        "    if any(f.endswith('.txt') for f in os.listdir(d)):\n",
        "        target_dir = d\n",
        "        break\n",
        "\n",
        "dataset_3_docs = []\n",
        "\n",
        "if target_dir:\n",
        "    print(f\"   Found local directory: {target_dir}\")\n",
        "    txt_files = glob.glob(os.path.join(target_dir, '*.txt'))\n",
        "    print(f\"   Found {len(txt_files)} text files\")\n",
        "\n",
        "    for txt_file in tqdm(txt_files, desc=\"Loading Kaggle files\"):\n",
        "        try:\n",
        "            with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "                if len(content) > 100:\n",
        "                    doc = Document(\n",
        "                        page_content=content,\n",
        "                        metadata={\n",
        "                            'source': 'kaggle_tosdr',\n",
        "                            'filename': os.path.basename(txt_file)\n",
        "                        }\n",
        "                    )\n",
        "                    dataset_3_docs.append(doc)\n",
        "        except Exception as e:\n",
        "            pass # Skip problematic files\n",
        "\n",
        "    print(f\"   - Kaggle ToSDR: {len(dataset_3_docs)} documents loaded\")\n",
        "else:\n",
        "    print(\"   âš ï¸  Kaggle dataset directory not found locally.\")\n",
        "    print(\"   (This is normal if you haven't manually uploaded the Kaggle zip file. We will proceed with the other 2 datasets.)\")\n",
        "\n",
        "print(f\"\\nDatasets loaded successfully!\")\n",
        "print(f\"   - OPP-115 Train: {len(dataset_1['train'])} examples\")\n",
        "print(f\"   - PrivacyPolicy Train: {len(dataset_2['train'])} examples\")\n",
        "\n",
        "# 4. CONVERT TO LANGCHAIN DOCUMENTS\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"PREPARING DOCUMENTS\")\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Process Dataset 1 (OPP-115)\n",
        "print(\"\\nProcessing OPP-115 dataset...\")\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    if split in dataset_1:\n",
        "        print(f\"  - Processing {split} split...\")\n",
        "        for i, item in enumerate(tqdm(dataset_1[split])):\n",
        "            doc = Document(\n",
        "                page_content=item['text'],\n",
        "                metadata={\n",
        "                    'source': 'opp_115',\n",
        "                    'split': split,\n",
        "                    'index': i,\n",
        "                    'original_id': f\"opp_{split}_{i}\"\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "\n",
        "# Process Dataset 2 (sjsq/PrivacyPolicy)\n",
        "print(\"\\nProcessing sjsq/PrivacyPolicy dataset...\")\n",
        "for split in dataset_2.keys():\n",
        "    print(f\"  - Processing {split} split...\")\n",
        "    for i, item in enumerate(tqdm(dataset_2[split])):\n",
        "        # Normalize column names (this dataset uses 'Text')\n",
        "        text_content = item.get('Text', item.get('text', ''))\n",
        "        if text_content and text_content != 'Null':\n",
        "             doc = Document(\n",
        "                page_content=text_content,\n",
        "                metadata={\n",
        "                    'source': 'sjsq_privacy_policy',\n",
        "                    'split': split,\n",
        "                    'index': i,\n",
        "                    'filename': item.get('filename', 'unknown')\n",
        "                }\n",
        "            )\n",
        "             documents.append(doc)\n",
        "\n",
        "# Append Kaggle documents\n",
        "if dataset_3_docs:\n",
        "    documents.extend(dataset_3_docs)\n",
        "\n",
        "print(f\"\\nTotal documents prepared: {len(documents)}\")\n",
        "\n",
        "# 5. CHUNK THE DOCUMENTS\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"CHUNKING DOCUMENTS FOR RETRIEVAL\")\n",
        "\n",
        "print(\"\\nSplitting text into searchable chunks...\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"\\nâœ… Created {len(chunks)} searchable chunks\")\n",
        "if len(chunks) > 0:\n",
        "    print(f\"   Average chunk size: ~{sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
        "\n",
        "# 6. CREATE EMBEDDINGS MODEL\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"LOADING EMBEDDING MODEL FROM HUGGING FACE\")\n",
        "\n",
        "print(\"\\nLoading: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- IMPORTANT: GPU CHECK ---\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"   - Using Device: {device.upper()}\")\n",
        "if device == 'cpu':\n",
        "    print(\"   âš ï¸ WARNING: Running on CPU. This will be slow (15+ mins). Switch runtime to GPU for speed.\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"\\nEmbedding model loaded successfully!\")\n",
        "\n",
        "# 7. BUILD VECTOR DATABASE\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"BUILDING FAISS VECTOR DATABASE\")\n",
        "\n",
        "print(\"\\nðŸ”¨ Creating embeddings for all chunks...\")\n",
        "if len(chunks) > 5000 and device == 'cpu':\n",
        "    print(\"   (You have many chunks and are using CPU. This might take 10-20 minutes.)\")\n",
        "else:\n",
        "    print(\"   (Processing...)\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "print(\"\\nVector database created successfully!\")\n",
        "print(f\"   Total vectors: {vectorstore.index.ntotal}\")\n",
        "\n",
        "# 8. TEST THE DATABASE\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"TESTING RETRIEVAL\")\n",
        "\n",
        "test_queries = [\n",
        "    \"Can the company sell my personal data to third parties?\",\n",
        "    \"Do they use my content to train AI models?\",\n",
        "    \"Can I delete all my data from their servers?\",\n",
        "    \"Am I forced into arbitration instead of court?\"\n",
        "]\n",
        "\n",
        "print(\"\\nRunning test queries...\\n\")\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"Query: '{query}'\")\n",
        "    results = vectorstore.similarity_search(query, k=2)\n",
        "    if results:\n",
        "        print(f\"Found {len(results)} relevant chunks\")\n",
        "        print(f\"   Top result preview: {results[0].page_content[:150]}...\")\n",
        "    else:\n",
        "        print(\"   No results found.\")\n",
        "    print()\n",
        "\n",
        "# 9. SAVE THE DATABASE\n",
        "print(\"=\" * 30)\n",
        "print(\"SAVING DATABASE\")\n",
        "\n",
        "print(\"\\nðŸ’¾ Saving FAISS database to disk...\")\n",
        "vectorstore.save_local(\"faiss_index_tos_hf\")\n",
        "\n",
        "print(\"Database saved to: faiss_index_tos_hf/\")\n",
        "\n",
        "# 10. CREATE METADATA FILE\n",
        "metadata = {\n",
        "    \"dataset\": \"OPP-115 + PrivacyPolicy\",\n",
        "    \"total_documents\": len(documents),\n",
        "    \"total_chunks\": len(chunks),\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"embedding_dimension\": 384,\n",
        "    \"chunk_size\": 800,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"vector_database\": \"FAISS\",\n",
        "    \"date_created\": \"2024-12\",\n",
        "    \"license\": \"Apache 2.0\",\n",
        "    \"use_case\": \"Terms of Service Analysis - IAT 360\"\n",
        "}\n",
        "\n",
        "with open('faiss_index_tos_hf/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Metadata saved\")\n",
        "\n",
        "# 11. ZIP FOR DOWNLOAD\n",
        "print(\"\\nCreating zip file for download...\")\n",
        "!zip -r -q faiss_index_tos_hf.zip faiss_index_tos_hf\n",
        "\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "print(\"SUCCESS! DATABASE READY FOR DEPLOYMENT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rwtakRLIFDZS",
      "metadata": {
        "id": "rwtakRLIFDZS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
